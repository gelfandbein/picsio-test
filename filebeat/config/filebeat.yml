
filebeat.inputs:
- type: docker
  combine_partial: true
  containers:
    path: "/usr/share/filebeat/docker/containers"
    stream: "stdout"
    ids:
      - "*"
  exclude_files: ['\.gz$']
  ignore_older: 10m

processors:
  # decode the log field (sub JSON document) if JSON encoded, then maps it's fields to elasticsearch fields
- decode_json_fields:
    fields: ["log", "message"]
    target: ""
    # overwrite existing target elasticsearch fields while decoding json fields    
    overwrite_keys: true
- add_docker_metadata:
    host: "unix:///var/run/docker.sock"

filebeat.config.modules:
  path: ${path.config}/modules.d/*.yml
  reload.enabled: false

##########################################
################################# OUTPUT #
##########################################
output.logstash:
  hosts: ["logstash"]

#output.elasticsearch:
#  hosts: ["elasticsearch:9200"]
#  indices:
#     - index: "filebeat-elastic-%{[agent.version]}-%{+yyyy.MM.dd}"
#  username: "elastic"
#  password: "changeme"


# Write Filebeat own logs only to file to avoid catching them with itself in docker log files
logging.level: error
logging.to_files: false
logging.to_syslog: false
loggins.metrice.enabled: false
logging.files:
  path: /usr/share/filebeat/logs
  name: filebeat
  keepfiles: 7
  permissions: 0644
ssl.verification_mode: none

#logging:
#xpack.monitoring:
#  enabled: true
#  elasticsearch:
#    hosts: ["elasticsearch:9200"]
#    username: elastic
#    password: changeme

