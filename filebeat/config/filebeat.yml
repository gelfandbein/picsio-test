#filebeat.prospectors:
#- type: log
#  paths:
#    - /usr/share/filebeat/logs/*.log

filebeat.config:
  modules:
    path: ${path.config}/modules.d/*.yml
    reload.enabled: false

#setup.template.name: "filebeat-"
#setup.template.pattern: "filebeat-*"

setup.dashboards.enabled: true

filebeat.autodiscover:
  providers:
    - type: docker
      hints.enabled: true

#filebeat.autodiscover:
#  providers:
#    - type: docker
#      hints.enabled: true
#      templates:
#        - condition:
#            contains:
#              docker.container.name: "nginx"
#          config:
#            - module: nginx
#              access:
#                input:
#                  type: docker
#                  containers:
#                    ids: "${data.docker.container.id}"
#                    stream: "stdout"
#              error:
#                input:
#                  type: docker
#                  containers:
#                    ids: "${data.docker.container.id}"
#                    stream: "stderr"


# work
#filebeat.modules:
#- module: nginx
#  access:
#    enabled: true
#    var.paths: ["/var/log/nginx/access.log"]
#  error:
#    enabled: true
#    var.paths: ["/var/log/nginx/error.log"]
#- module: system
#- module: elasticsearch
#- module: kibana
#- module: logstash
#  log:
#    enabled: true
#    var.paths: ["/var/log/logstash/logstash.log"]
#  slowlog:
#    enabled: true
#    var.paths:

#filebeat.prospectors:
#        -
#            paths:
#                - "/etc/logs/nginx/nginx_access.log"
#            document_type: nginx-access
#        -
#            paths:
#                - "/etc/logs/nginx/nginx_error.log"
#            document_type: nginx-error


# work
filebeat.inputs:
  - type: container
    enabled: true
    ignore_older: 10m
    paths:
      - "/var/lib/docker/containers/*/*.log"
    tags: ["docker-logs"]
#  - type: docker
#    combine_partial: true
#    containers:
#      path: "/var/lib/docker/containers"
#      stream: "stdout"
#      ids:
#        - "*"
#    exclude_files: ['\.gz$']
#    ignore_older: 10m

#processors:
#  # decode the log field (sub JSON document) if JSON encoded, then maps it's fields to elasticsearch fields
#- decode_json_fields:
#    fields: ["log", "message"]
#    target: ""
#    # overwrite existing target elasticsearch fields while decoding json fields    
#    overwrite_keys: true
#- add_docker_metadata:
#    host: "unix:///var/run/docker.sock"


#http.enabled: true
#http.port: 5066
#monitoring.enabled: false

# Write Filebeat own logs only to file to avoid catching them with itself in docker log files
logging.level: error
logging.to_syslog: false
#loggins.metrice.enabled: false
### Disable logging filebeat internal metrics
#logging.metrics.enabled: true

#logging.to_files: false
#logging.files:
#  path: /usr/share/filebeat/logs
#  name: filebeat
#  keepfiles: 7
#  permissions: 0644
#ssl.verification_mode: none

##########################################
################################# OUTPUT #
##########################################
#output.logstash:
#  hosts: ["logstash:5044"]
#  loadbalance: true
#  indices:
#    - index: "filebeat-%{+yyyy.MM.dd}"
#  username: "elastic"
#  password: "changeme"


#xpack.monitoring.enabled: true
#xpack.monitoring.elasticsearch:
#  hosts: ["http://elasticsearch:9200"]
#  username: elastic
#  password: changeme

### OUT ONLY TO ELASTICSEARCH !!!
output.elasticsearch:
  hosts: ["elasticsearch:9200"]
  indices:
     - index: "filebeat-%{+yyyy.MM.dd}"
  username: "elastic"
  password: "changeme"

setup.kibana:
  host: http://kibana:5601
  username: elastic
  password: changeme

