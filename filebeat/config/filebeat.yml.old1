#filebeat.prospectors:
#- type: log
#  paths:
#    - /usr/share/filebeat/logs/*.log

filebeat.config:
  modules:
    path: ${path.config}/modules.d/*.yml
    reload.enabled: false

#setup.template.name: "filebeat-"
#setup.template.pattern: "filebeat-*"

setup.dashboards.enabled: true

filebeat.autodiscover:
  providers:
    - type: docker
      hints.enabled: true
      templates:
        - condition:
            contains:
              docker.container.name: "nginx"
          config:
            - module: nginx
              access:
                input:
                  type: docker
                  containers:
                    ids: "${data.docker.container.id}"
                    stream: "stdout"
              error:
                input:
                  type: docker
                  containers:
                    ids: "${data.docker.container.id}"
                    stream: "stderr"


# work
#filebeat.modules:
#- module: nginx
#  access:
#    enabled: true
#    var.paths: ["/var/log/nginx/access.log"]
#  error:
#    enabled: true
#    var.paths: ["/var/log/nginx/error.log"]
#- module: system
#- module: elasticsearch
#- module: kibana
#- module: logstash
#  log:
#    enabled: true
#    var.paths: ["/var/log/logstash/logstash.log"]
#  slowlog:
#    enabled: true
#    var.paths:

#filebeat.prospectors:
#        -
#            paths:
#                - "/etc/logs/nginx/nginx_access.log"
#            document_type: nginx-access
#        -
#            paths:
#                - "/etc/logs/nginx/nginx_error.log"
#            document_type: nginx-error


# work
#filebeat.inputs:
#- type: docker
#  combine_partial: true
#  containers:
#    path: "/usr/share/filebeat/docker/containers"
#    stream: "stdout"
#    encoding: plain
#    ids:
#      - "*"
#  exclude_files: ['\.gz$']
#  ignore_older: 10m

processors:
  # decode the log field (sub JSON document) if JSON encoded, then maps it's fields to elasticsearch fields
- decode_json_fields:
    fields: ["log", "message"]
    target: ""
    # overwrite existing target elasticsearch fields while decoding json fields    
    overwrite_keys: true
- add_docker_metadata:
    host: "unix:///var/run/docker.sock"

##########################################
################################# OUTPUT #
##########################################
output.logstash:
  hosts: ["logstash"]
  loadbalance: true
  indices:
    - index: "filebeat-logstash-%{+yyyy.MM.dd}"
  username: "elastic"
  password: "changeme"


### OUT ONLY TO ELASTICSEARCH !!!
#xpack.monitoring.enabled: true
#output.elasticsearch:
#  hosts: ["elasticsearch:9200"]
#  indices:
#     - index: "filebeat-elasticsearch-%{+yyyy.MM.dd}"
#  username: "elastic"
#  password: "changeme"


# Write Filebeat own logs only to file to avoid catching them with itself in docker log files
logging.level: error
logging.to_syslog: false
loggins.metrice.enabled: false
### Disable logging filebeat internal metrics
logging.metrics.enabled: true

#logging.to_files: false
#logging.files:
#  path: /usr/share/filebeat/logs
#  name: filebeat
#  keepfiles: 7
#  permissions: 0644
#ssl.verification_mode: none

# logging:
### Uses ONLY WITH LOGSTASH !!!
xpack.monitoring:
  enabled: true
  elasticsearch:
    hosts: ["elasticsearch:9200"]
    username: elastic
    password: changeme

setup.kibana:
  host: http://kibana:5601
  username: elastic
  password: changeme
