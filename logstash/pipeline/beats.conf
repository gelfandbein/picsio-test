###
# tcp -> beats (for charset ISO-8859-1)
# + manage_template => false
# - index => "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}"
# - host => 0.0.0.0

# -	index => "%{[@metadata][beat]}-%{+YYYY.MM.dd}"
#        codec => plain {
#	    charset => "ISO-8859-1"
#        }

input {
    beats {
        port => 5044
    }
}

## Add your filters / logstash plugins configuration here
#filter {
#    if [type] == "docker" {
#    [..............................................]
#        mutate { remove_field => [ "time" ] }
#    }
#}

#        mutate {
#          rename => ["host", "server"]
#          convert => {"server" => "string"}
#        }

### worked version
#filter {
#  if [system][process] {
#    if [system][process][cmdline] {
#      grok {
#        match => {
#          "[system][process][cmdline]" => "^%{PATH:[system][process][cmdline_path]}"
#        }
#        remove_field => "[system][process][cmdline]"
#      }
#    }
#  }
#}

#    filter {
#      if [type] == "nginx-access" {
#        grok {
#          match => { "message" => "%{NGINXACCESS}" }
#        }
#      }
#    }

#  if "postfix" in [tags]{
#          elasticsearch {
#              hosts    => "localhost:9200"
#              index    => "postfix-%{+YYYY.MM.dd}"
#          }
#  }



#filter {
#  if [type] == "syslog" {
#    grok {
#      match => { "message" => "%{SYSLOG_DEBIAN}" }
#      add_field => [ "received_at", "%{@timestamp}" ]
#      add_field => [ "received_from", "%{host}" ]
#    }
#    date {
#      match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
#    }
#  }
#
#  else if [type] == "nginx-access" {
#    grok {
#      patterns_dir => ["/usr/share/logstash/patterns"]
#      match => { "message" => "%{NGINX_ACCESS}" }
#      add_field => [ "received_from", "%{host}" ]
#    }
#    ruby {
#      code => "
#        event.set('request_time_ms', event.get('request_time') * 1000)
#        event.set('upsteam_response_time_ms', event.get('upsteam_response_time') * 1000)
#      "
#    }
#    mutate {
#      remove_field => [ "request_time", "upsteam_response_time" ]
#      convert => {
#        "request_time_ms" => "integer"
#        "upsteam_response_time_ms" => "integer"
#      }
#    }
#    geoip {
#      source => "client_ip"
#    }
#    useragent {
#      source => "useragent_raw"
#      target => "useragent"
#    }
#  }
#}

filter {
    grok {
    ### telegraf makes timestamps in microseconds but logstash supports only miliseconds so it was necessary to cut
    ### timestamp from 19 digits to 16 digits that is why there is d{13} not d{19} in below grok regex
        match => { "message" => "(?<date>(timestamp\":)\d{13})"}
    }
    mutate {
        gsub => [ "date", "timestamp\":", "" ]
    }
    mutate {
        rename => ["host", "server"]
        convert => {"server" => "string"}
    }
    date {
        match => [ "date","UNIX_MS" ]
        target => "@timestamp"
    }
#    json {
#    source => "message"
#    }
#    if "counter" in [tags] {
#        mutate {
#            add_field => {
#                "subname" => "%{[tags][counter]}"
#            }
#        }
#    }
#    ruby {
#        code => 'event.set("tagsAsInfo", event.get("tags").to_h)'
#    }
}



output {
  elasticsearch {
    hosts => "http://elasticsearch:9200"
    user => "elastic"
    password => "changeme"
    manage_template => false
    ### If ILM is not being used, use this index, instead so Logstash creates an index per day,
    ### based on the @timestamp value of the events coming from Beats.
    index => "logstash-beats-%{+YYYY.MM.dd}"
#    index => "%{[@metadata][beat]}-%{[@metadata]}-%{+YYYY.MM.dd}"
#    document_type => "%{[@metadata][type]}"
  }
  stdout {
    codec => plain {
      charset => "UTF-8"
#      charset => "ISO-8859-1"
    }
  }

#  stdout { codec => dots }
#  stdout { codec => rubydebug }

}

